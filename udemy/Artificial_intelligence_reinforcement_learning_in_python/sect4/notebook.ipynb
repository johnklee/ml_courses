{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b239aaee",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Markov Decision Process (KDP) Introduction</font>\n",
    "([course link](https://www.udemy.com/course/artificial-intelligence-reinforcement-learning-in-python/learn/lecture/19908102#overview)) <font size='3ptx'><b>The framework from which everything else is derived. </b></font> (<font color='brown'>Q-learning, deep Q-learning, etc.</font>)\n",
    "\n",
    "![RL state](images/1.PNG)\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2994ca28",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>The Markov Proberty/Markov Models</font>\n",
    "* Obviously necessary to learn about MDP\n",
    "* Appears in a wide variety of fields.\n",
    "* Google's PageRank algorithm\n",
    "* Hidden Markov Models (speech recognition, genomics)\n",
    "* MCMC/Bayesian Machine Learning\n",
    "* Finance (Random Walk Hypothesis)\n",
    "* Control systems (Electrical/Mechanical Engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99d7d91",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Building on MDPs</font>\n",
    "* Returns (sum of future rewards)\n",
    "* Value (expected sum of future rewards)\n",
    "* Bellman equation\n",
    "* Solving for the value function (what this entire course boils down to)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdcb994",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>GridWorld</font>\n",
    "([course link](https://www.udemy.com/course/artificial-intelligence-reinforcement-learning-in-python/learn/lecture/6408554#overview))\n",
    "* The perfect-sized environment to learn about RL\n",
    "* Image it like a video game (e.g.: Pacman)\n",
    "\n",
    "![Gridworld](images/2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236e538d",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Gridworld States</font>\n",
    "Recall: in programming, we count rows going down and columns going right.\n",
    "\n",
    "![gridworld states](images/3.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eedd384",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Environment</font>\n",
    "* The \"world\" that the agent lives in\n",
    "* Examples: Gridworld, Super Mario, Pong, Chess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf1c584",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Policy</font>\n",
    "* A \"function\" that maps state to action (the agent's brain)\n",
    "* Notice: there are 2 \"equally good\" options from the initial state\n",
    "* Policy represesntation:\n",
    "    * Deterministic: $π(s)$\n",
    "    * Probablistic: $π(a|s)$\n",
    "* Action space in below environment is 4 (Up/Down/Left/Right)    \n",
    "\n",
    "![policy](images/4.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844e6535",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Choosing Rewards</font>\n",
    "([course link](https://www.udemy.com/course/artificial-intelligence-reinforcement-learning-in-python/learn/lecture/19908136#overview))\n",
    "* In Gridworld: +1 at winning state, -1 at losing state\n",
    "* Regards are engineered by the user\n",
    "* Consider a maze. +1 reward for finding maze exit? Or -1 reward for each step?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a01a323",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>The Markov Property</font>\n",
    "([course link](https://www.udemy.com/course/artificial-intelligence-reinforcement-learning-in-python/learn/lecture/6408556#overview)) <b><font size='3ptx'>\"All models are wrong, but some are useful\" - Box, George E. P.; Norman R. Draper (1987) </font></b>\n",
    "* Forget about any sequence longer than 2\n",
    "* Bigram - The first-older Markov assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb1d674",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>The MDPs</font>\n",
    "([course link](https://www.udemy.com/course/artificial-intelligence-reinforcement-learning-in-python/learn/lecture/6408558#overview)) <b><font size='3ptx'>MDP: A discrete-time stochastic control process</font></b>\n",
    "\n",
    "![MDP](images/5.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45ffdbd",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Future Rewards</font>\n",
    "([course link](https://www.udemy.com/course/artificial-intelligence-reinforcement-learning-in-python/learn/lecture/6408560#overview))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145cac92",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Value Functions</font>\n",
    "([course link](https://www.udemy.com/course/artificial-intelligence-reinforcement-learning-in-python/learn/lecture/6408562#overview))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
